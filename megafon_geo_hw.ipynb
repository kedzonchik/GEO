{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from geopy import distance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from shapely.geometry import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача:\n",
    "\n",
    "Прогноз продаж одной из популярных моделей [фичерфонов](https://ru.wikipedia.org/wiki/%D0%A4%D0%B8%D1%87%D0%B5%D1%80%D1%84%D0%BE%D0%BD) (на картинке ниже пример похожего устройства) в салонах МегаФона\n",
    "![](https://39.img.avito.st/640x480/8468720439.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные данные:\n",
    "\n",
    "Датасет содержит следующие поля:\n",
    "\n",
    "1. `point_id` - Индентификатор салона\n",
    "2. `lon` - Долгота точки\n",
    "3. `lat` - Широта точки\n",
    "4. `target` - Значение таргета, усредненное за несколько месяцев и отнормированное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Требования к решению и советы:\n",
    "\n",
    "Ниже приведен список из нескольких важных пунктов, необходимых для решения задания. Выполнение каждого из пунктов влияет на итоговую оценку. Вы можете выполнить каждый из пунктов разными способами, самым лучшим будет считаться вариант, когда всё получение и обработка данных будут реализованы на Питоне (пример: вы можете скачать данные из OSM через интерфейс на сайте overpass-turbo или с помощью библиотек `overpass`/`requests`. Оба варианта будут зачтены, но больше баллов можно заработать во втором случае)\n",
    "\n",
    "\n",
    "\n",
    "1. Салоны расположены в нескольких разных городах, вам необходимо **определить город для каждого салона** (это понадобится во многих частях задания). К этому есть разные подходы. Вы можете провести [обратное геокодирование](https://en.wikipedia.org/wiki/Reverse_geocoding) с помощью геокодера [Nominatim](https://nominatim.org/), доступного через библиотеку `geopy` примерно вот так:\n",
    "```python\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "location = geolocator.reverse(\"52.509669, 13.376294\")\n",
    "print(location.address)\n",
    "```\n",
    "В таком случае, вам придется обрабатывать полученную строку адреса, чтобы извлечь название города. Также вы можете скачать из OSM или найти в любом другом источнике границы административно территориальных границ России и пересечь с ними датасет с помощью `geopandas.sjoin` (этот вариант более надежный, но нужно будет разобраться с тем, как устроены границы АТД в OSM, обратите внимание на [этот тег](https://wiki.openstreetmap.org/wiki/Key:admin_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Используйте данные OSM**: подумайте, какие объекты могут влиять на продажи фичерфонов. Гипотеза: такие телефоны покупают люди, приезжающие в город или страну ненадолго, чтобы вставить туда отдельную симкарту для роуминга. Можно попробовать использовать местоположения железнодорожных вокзалов (изучите [этот тег](https://wiki.openstreetmap.org/wiki/Tag:railway%3Dstation)). Необходимо использовать хотя бы 5 разных типов объектов из OSM. Скорее всего, вам придется качать данные OSM отдельно для разных городов (см. пример для Нью-Йорка из лекции)\n",
    "\n",
    "\n",
    "3. **Используйте разные способы генерации признаков**: описать положение салона МегаФона относительно станций метро можно разными способами - найти ***расстояние до ближайшей станции***, или же посчитать, сколько станций попадает в ***500 метровую буферную зону*** вокруг салона. Такие признаки будут нести разную информацию. Так же попробуйте поэкспериментировать с размерами буферных зон (представьте, что значат в реальности радиусы 100, 500, 1000 метров). Попробуйте посчитать расстояние до центра города, до других объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Сделайте визуализации**: постройте 2-3 карты для какого нибудь из городов - как распределен в пространстве таргет, где находятся объекты, полученные вами из OSM. Можете использовать любой инструмент - обычный `plot()`, `folium`, `keplergl`. Если выберете Кеплер, обязательно сохраните в файл конфиг карты, чтобы ее можно было воспроизвести. Сделать это можно вот так:\n",
    "\n",
    "```python\n",
    "import json\n",
    "json_data = kepler_map.config\n",
    "with open('kepler_config.json', 'w') as outfile:\n",
    "    json.dump(json_data, outfile)\n",
    "```\n",
    "5. Задание не ограничено приведенными выше пунктами, попробуйте нагенерировать интересных признаков, найти в интернете дополнительные данные (в таком случае в комментарии к коду укажите ссылку на ресурс, откуда взяли данные)\n",
    "\n",
    "\n",
    "\n",
    "6. Это довольно сложная задача - датасет очень маленький, данные по своей природе довольно случайны. Поэтому место и скор на Kaggle не будут играть решающую роль в оценке, но позволят заработать дополнительные баллы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/mf_geo_train.csv')\n",
    "test = pd.read_csv('data/mf_geo_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin4 = gpd.read_file(\"data/admin_level_4.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin6 = gpd.read_file(\"data/admin_level_6.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_to_geo(df):\n",
    "    df1 = df.reset_index().drop(columns='index')\n",
    "    geometry = [shapely.geometry.asPoint((row.lon,row.lat)) for index,row in df1.iterrows()]\n",
    "    df_buf = df1.drop(columns=['lon','lat'])\n",
    "    result_df = gpd.GeoDataFrame(pd.concat([df_buf,pd.DataFrame({'geometry' : geometry})],axis=1))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_to_geo(train).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd_to_geo(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_loc(df):\n",
    "    df_admin6 = gpd.sjoin(pd_to_geo(df),admin6[['name','geometry']],op='within')\n",
    "    df_admin4 = gpd.sjoin(pd_to_geo(df),admin4[['name','geometry']])\n",
    "    df_loc = df_admin6.append(df_admin4.drop(index=df_admin6.index)).reset_index().drop(columns=['index','index_right']).rename(columns={'name':'location'})\n",
    "    return df_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shape(df):\n",
    "    geometry_100 = []\n",
    "    geometry_500 = []\n",
    "    geometry_1000 = []\n",
    "    for index,x in df.iterrows():\n",
    "        geometry_100.append(geodesic_point_buffer(x.lon,x.lat,0.1))\n",
    "        geometry_500.append(geodesic_point_buffer(x.lon,x.lat,0.5))\n",
    "        geometry_1000.append(geodesic_point_buffer(x.lon,x.lat,1))\n",
    "    df_shape = pd.concat([df,pd.DataFrame({'100':geometry_100}),pd.DataFrame({'500':geometry_500}),pd.DataFrame({'1000':geometry_1000})],axis=1)\n",
    "    return df_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = df_loc(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loc = df_loc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[55.772910,37.678790],zoom_start=8)\n",
    "folium.GeoJson(admin5[['name','geometry']][admin5.name.isin(train_loc.location)].geometry).add_to(m)\n",
    "folium.GeoJson(admin4[['name','geometry']][admin4.name.isin(train_loc.location)].geometry).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for index,row in train.iterrows():\n",
    "    folium.Marker([row.lat,row.lon],popup=row.target).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "import requests\n",
    "proj_wgs84 = pyproj.Proj('+proj=longlat +datum=WGS84')\n",
    "\n",
    "def geodesic_point_buffer(lon, lat, km):\n",
    "    # Azimuthal equidistant projection\n",
    "    aeqd_proj = '+proj=aeqd +lat_0={lat} +lon_0={lon} +x_0=0 +y_0=0'\n",
    "    project = partial(\n",
    "        pyproj.transform,\n",
    "        pyproj.Proj(aeqd_proj.format(lon=lon, lat=lat)),\n",
    "        proj_wgs84)\n",
    "    buf = Point(0, 0).buffer(km * 1000)  # distance in metres\n",
    "    return transform(project, buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_comp(name):\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = '''\n",
    "    [out:json];\n",
    "    (area[\"name\"=\"{}\"];) -> .search;\n",
    "    (node[\"shop\"=\"mobile_phone\"](area.search);) -> .node;\n",
    "    .node out geom;\n",
    "    '''.format(name)\n",
    "\n",
    "    response = requests.get(overpass_url, \n",
    "                        params={'data': overpass_query})\n",
    "    amenity = response.json()\n",
    "    amenity_osm = pd.DataFrame(amenity['elements'])\n",
    "    amenity_osm = amenity_osm.join(\n",
    "        pd.DataFrame([x['tags'] for x in amenity['elements']])).drop('tags', axis=1)\n",
    "    try:\n",
    "        comp = amenity_osm.name.value_counts().head(6).drop(index=['МегаФон','Мегафон']).index\n",
    "    except: \n",
    "        try: \n",
    "            comp = amenity_osm.name.value_counts().head(6).drop(index=['МегаФон']).index\n",
    "        except:\n",
    "            try:\n",
    "                comp = amenity_osm.name.value_counts().head(6).drop(index=['Мегафон']).index\n",
    "            except:\n",
    "                comp = amenity_osm.name.value_counts().head(6).index\n",
    "    comp_sal = amenity_osm[amenity_osm[['name']].isin(comp).values]\n",
    "    \n",
    "    return comp_sal.dropna(axis=1)[['lat','lon','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_comp(df):\n",
    "    df1 = pd.DataFrame()\n",
    "    for i in df.location.value_counts().index:\n",
    "        print(i)\n",
    "        df1 = pd.concat([df1,city_comp(i)],axis=0)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp(df_loc(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df100(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'100':'geometry'})),\n",
    "            pd_to_geo(df_comp(df_loc(df))[['lat','lon','name']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['name'].count()],axis=1).reset_index().rename(columns={'name':'salons'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df500(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'500':'geometry'})),\n",
    "            pd_to_geo(df_comp(df_loc(df))[['lat','lon','name']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['name'].count()],axis=1).reset_index().rename(columns={'name':'salons'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_station(name):\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = '''\n",
    "    [out:json];\n",
    "    (area[\"name\"=\"{}\"];) -> .search;\n",
    "    (node[\"railway\"=\"station\"](area.search);) -> .node;\n",
    "    .node out geom;\n",
    "    '''.format(name)\n",
    "\n",
    "    response = requests.get(overpass_url, \n",
    "                        params={'data': overpass_query})\n",
    "    amenity = response.json()\n",
    "    amenity_osm = pd.DataFrame(amenity['elements'])\n",
    "    amenity_osm = amenity_osm.join(\n",
    "        pd.DataFrame([x['tags'] for x in amenity['elements']])).drop('tags', axis=1)\n",
    "    \n",
    "    \n",
    "    return amenity_osm[['lat','lon','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=[55.772910,37.678790],zoom_start=8)\n",
    "for index, row in city_station('Москва').iterrows():\n",
    "    folium.Marker([row.lat,row.lon],icon=folium.Icon(color='green')).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(df,st):\n",
    "    gst = pd_to_geo(st)\n",
    "    dist1 = []\n",
    "    for point in df.point_id:\n",
    "        a = []\n",
    "        point_coord = df[df.point_id==point].geometry.values.y, df[df.point_id==point].geometry.values.x\n",
    "        for index,p in gst.iterrows():\n",
    "            station_coord = p.geometry.y,p.geometry.x\n",
    "            a.append(distance.geodesic(point_coord,station_coord).m)\n",
    "        dist1.append(np.array(a).min())\n",
    "\n",
    "    return(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_dis(df):\n",
    "    df_dis1 = df_loc(df).copy()\n",
    "    for i, x in enumerate(df_dis1.location.value_counts().index):\n",
    "         df_dis1.loc[df_dis1.location == x, 'railway_dist'] = dist(df_dis1[df_dis1.location == x],city_station(x))\n",
    "    return df_dis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dis(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sta(df):\n",
    "    df1 = pd.DataFrame()\n",
    "    for i in df.location.value_counts().index:\n",
    "        print(i)\n",
    "        df1 = pd.concat([df1,city_station(i)],axis=0)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df100_2(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'100':'geometry'})),\n",
    "            pd_to_geo(df_sta(df_loc(df))[['lat','lon','name']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['name'].count()],axis=1).reset_index().rename(columns={'name':'stations'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df500_2(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'500':'geometry'})),\n",
    "            pd_to_geo(df_sta(df_loc(df))[['lat','lon','name']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['name'].count()],axis=1).reset_index().rename(columns={'name':'stations'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df1000_2(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'1000':'geometry'})),\n",
    "            pd_to_geo(df_sta(df_loc(df))[['lat','lon','name']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['name'].count()],axis=1).reset_index().rename(columns={'name':'stations'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df100_2(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df500_2(train)[0].stations.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1000_2(train)[0].stations.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index,x in df_shape(train).iterrows():\n",
    "    folium.GeoJson(x['1000'],style_function=lambda x :{'fillColor':'red'}).add_to(m)\n",
    "    folium.GeoJson(x['500'],style_function=lambda x :{'fillColor':'black'}).add_to(m)\n",
    "    folium.GeoJson(x['100']).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_mall(name):\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = '''\n",
    "    [out:json];\n",
    "    (area[\"name\"=\"{}\"];) -> .search;\n",
    "    (node[\"shop\"=\"mall\"](area.search);) -> .node;\n",
    "    .node out count;\n",
    "    .node out geom;\n",
    "    '''.format(name)\n",
    "\n",
    "    response = requests.get(overpass_url, \n",
    "                        params={'data': overpass_query})\n",
    "    amenity = response.json()\n",
    "    amenity_osm = pd.DataFrame(amenity['elements'])\n",
    "    amenity_osm = amenity_osm.join(\n",
    "        pd.DataFrame([x['tags'] for x in amenity['elements']])).drop('tags', axis=1)\n",
    "    try:\n",
    "        comp = amenity_osm.name.value_counts().head(6).drop(index=['МегаФон','Мегафон']).index\n",
    "    except: \n",
    "        try: \n",
    "            comp = amenity_osm.name.value_counts().head(6).drop(index=['МегаФон']).index\n",
    "        except:\n",
    "            try:\n",
    "                comp = amenity_osm.name.value_counts().head(6).drop(index=['Мегафон']).index\n",
    "            except:\n",
    "                comp = amenity_osm.name.value_counts().head(6).index\n",
    "    comp_sal = amenity_osm[amenity_osm[['name']].isin(comp).values]\n",
    "    \n",
    "    return comp_sal.dropna(axis=1)[['lat','lon','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_mall(df):\n",
    "    df1 = pd.DataFrame()\n",
    "    for i in df.location.value_counts().index:\n",
    "        print(i)\n",
    "        df1 = pd.concat([df1,city_mall(i)],axis=0)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df100_3(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'1000':'geometry'})),\n",
    "            pd_to_geo(df_mall(df_loc(df))[['lat','lon','name']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['name'].count()],axis=1).reset_index().rename(columns={'name':'malls'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df100_3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_sleep(name):\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = '''\n",
    "    [out:json];\n",
    "    (area[\"name\"=\"{}\"];) -> .search;\n",
    "    (node[\"amenity\"=\"school\"](area.search);) -> .node1;\n",
    "    (node[\"amenity\"=\"hospital\"](area.search);) -> .node2;\n",
    "    (node[\"leisure\"=\"playground\"](area.search);) -> .node;\n",
    "    .node out geom;\n",
    "    .node1 out geom;\n",
    "    .node2 out geom;\n",
    "    '''.format(name)\n",
    "\n",
    "    response = requests.get(overpass_url, \n",
    "                        params={'data': overpass_query})\n",
    "    amenity = response.json()\n",
    "    amenity_osm = pd.DataFrame(amenity['elements'])\n",
    "    amenity_osm = amenity_osm.join(\n",
    "        pd.DataFrame([x['tags'] for x in amenity['elements']])).drop('tags', axis=1)\n",
    "    \n",
    "    return amenity_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sleep(df):\n",
    "    df1 = pd.DataFrame()\n",
    "    for i in df.location.value_counts().index:\n",
    "        print(i)\n",
    "        df1 = pd.concat([df1,city_sleep(i)],axis=0)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df1000_4(df):\n",
    "    df100 = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(\n",
    "            df_shape(df).rename(columns={'1000':'geometry'})),\n",
    "            pd_to_geo(df_sleep(df_loc(df))[['lat','lon','id']]),how='left')\n",
    "    df_2f_100 = pd.concat(\n",
    "        [df_loc(df).drop(columns='target').set_index('point_id'),\n",
    "         df100.groupby('point_id')['id'].count()],axis=1).reset_index().rename(columns={'id':'infrastructure'}) \n",
    "    X = df_2f_100.set_index('index').drop(columns='geometry')\n",
    "    map1 = dict(zip(X.location.value_counts().index,[x for x in range(0,len(X.location.value_counts().index))]))\n",
    "    X.location = X.location.map(map1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salons = df100(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = df500_2(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malls = df100_3(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = df1000_4(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distant = df_dis(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distant.drop(columns=['target','geometry','location']).set_index('point_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([sleep,malls.drop(columns='location'),stations.drop(columns='location'),salons.drop(columns='location'),\n",
    "                     distant.drop(columns=['target','geometry','location']).set_index('point_id')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salons_test = df100(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_test = df500_2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malls_test = df100_3(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_test = df1000_4(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_test = df_dis(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_test.drop(columns=['target','geometry','location']).set_index('point_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([sleep_test,malls_test.drop(columns='location'),stations_test.drop(columns='location'),salons_test.drop(columns='location'),\n",
    "                    dist_test.drop(columns=['target','geometry','location']).set_index('point_id')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.drop(columns=['lon','lat']).set_index('point_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(learning_rate=0.01,max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(learning_rate=0.01,max_depth=2).fit(train_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1*cross_val_score(xg,train_df,y=y,cv=20,scoring='neg_mean_absolute_error').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1*cross_val_score(gb,train_df,y=y,cv=20,scoring='neg_mean_absolute_error').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df, y)\n",
    "model = LinearRegression().fit(train_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model,train_df,y,cv=20,scoring='neg_mean_absolute_error').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_valid, model.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(train_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['target'] = model.predict(test_df)\n",
    "submission.to_csv('data/my_submission_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
